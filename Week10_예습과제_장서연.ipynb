{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‹ íŒŒì´í† ì¹˜ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í™œìš©í•œ ìì—°ì–´ ì²˜ë¦¬ì™€ ì»´í“¨í„°ë¹„ì „ ì‹¬ì¸µí•™ìŠµ"
      ],
      "metadata": {
        "id": "wSMQfP7-n2TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "f8pZVm1YRcdl",
        "outputId": "c9e7372d-07ff-4b1b-aa50-dd295158ce4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6b1084cf-1d59-401c-91b1-d5b3bbfc1d97\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6b1084cf-1d59-401c-91b1-d5b3bbfc1d97\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "CNv5FD9hRkZ1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d s076923/pytorch-transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92br_FadRn9v",
        "outputId": "3f31a63d-ce03-4e2a-e8f9-418c96c64944"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/s076923/pytorch-transformer\n",
            "License(s): other\n",
            "Downloading pytorch-transformer.zip to /content\n",
            " 97% 885M/916M [00:04<00:00, 123MB/s] \n",
            "100% 916M/916M [00:04<00:00, 220MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.unpack_archive(\n",
        "    filename=\"pytorch-transformer.zip\",\n",
        "    extract_dir=\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/\",\n",
        "    format=\"zip\"\n",
        ")"
      ],
      "metadata": {
        "id": "Mezo_Ij7RkTM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  5ì¥. í† í°í™”"
      ],
      "metadata": {
        "id": "7xJLAU6wEyXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.01 ë‹¨ì–´ í† í°í™”"
      ],
      "metadata": {
        "id": "DcnDTFcvFNBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review = \" í˜„ì‹¤ê³¼ êµ¬ë¶„ ë¶ˆê°€ëŠ¥í•œ cg. ì‹œê°ì  ì¦ê±°ìŒì€ ìµœê³ ! ë”ë¶ˆì–´ ostëŠ” ë”ë”ìš± ìµœê³ !!\"\n",
        "tokenized = review.split()\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bele8CcHFkM0",
        "outputId": "e56098aa-fbaf-4a58-cec8-7f36055504ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['í˜„ì‹¤ê³¼', 'êµ¬ë¶„', 'ë¶ˆê°€ëŠ¥í•œ', 'cg.', 'ì‹œê°ì ', 'ì¦ê±°ìŒì€', 'ìµœê³ !', 'ë”ë¶ˆì–´', 'ostëŠ”', 'ë”ë”ìš±', 'ìµœê³ !!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.02 ê¸€ì í† í°í™”"
      ],
      "metadata": {
        "id": "4_JenyheFTSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"í˜„ì‹¤ê³¼ êµ¬ë¶„ ë¶ˆê°€ëŠ¥í•œ cg. ì‹œê°ì  ì¦ê±°ìŒì€ ìµœê³ ! ë”ë¶ˆì–´ ostëŠ” ë”ë”ìš± ìµœê³ !!\"\n",
        "tokenized = list(review)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVGRTVf-G8Z0",
        "outputId": "6448a14c-518c-4739-cd9b-58fc707cfc2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['í˜„', 'ì‹¤', 'ê³¼', ' ', 'êµ¬', 'ë¶„', ' ', 'ë¶ˆ', 'ê°€', 'ëŠ¥', 'í•œ', ' ', 'c', 'g', '.', ' ', 'ì‹œ', 'ê°', 'ì ', ' ', 'ì¦', 'ê±°', 'ìŒ', 'ì€', ' ', 'ìµœ', 'ê³ ', '!', ' ', 'ë”', 'ë¶ˆ', 'ì–´', ' ', 'o', 's', 't', 'ëŠ”', ' ', 'ë”', 'ë”', 'ìš±', ' ', 'ìµœ', 'ê³ ', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.03 ìì†Œë‹¨ìœ„ í† í°í™”"
      ],
      "metadata": {
        "id": "bP1piqtxFTWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jamo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB6lWPWt61EQ",
        "outputId": "d1d49c9b-5a39-4462-e9d7-c442d693bb5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jamo in /usr/local/lib/python3.12/dist-packages (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jamo import h2j, j2hcj\n",
        "\n",
        "review = \"í˜„ì‹¤ê³¼ êµ¬ë¶„ ë¶ˆê°€ëŠ¥í•œ cg. ì‹œê°ì  ì¦ê±°ìŒì€ ìµœê³ ! ë”ë¶ˆì–´ ostëŠ” ë”ë”ìš± ìµœê³ !!\"\n",
        "decomposed = j2hcj(h2j(review))\n",
        "tokenized = list(decomposed)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3x_CtC5HJqN",
        "outputId": "e836a320-f09c-4302-f232-71b5504c68bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ã…', 'ã…•', 'ã„´', 'ã……', 'ã…£', 'ã„¹', 'ã„±', 'ã…˜', ' ', 'ã„±', 'ã…œ', 'ã…‚', 'ã…œ', 'ã„´', ' ', 'ã…‚', 'ã…œ', 'ã„¹', 'ã„±', 'ã…', 'ã„´', 'ã…¡', 'ã…‡', 'ã…', 'ã…', 'ã„´', ' ', 'c', 'g', '.', ' ', 'ã……', 'ã…£', 'ã„±', 'ã…', 'ã„±', 'ã…ˆ', 'ã…“', 'ã„±', ' ', 'ã…ˆ', 'ã…¡', 'ã„¹', 'ã„±', 'ã…“', 'ã…‡', 'ã…¡', 'ã…', 'ã…‡', 'ã…¡', 'ã„´', ' ', 'ã…Š', 'ã…š', 'ã„±', 'ã…—', '!', ' ', 'ã„·', 'ã…“', 'ã…‚', 'ã…œ', 'ã„¹', 'ã…‡', 'ã…“', ' ', 'o', 's', 't', 'ã„´', 'ã…¡', 'ã„´', ' ', 'ã„·', 'ã…“', 'ã„·', 'ã…“', 'ã…‡', 'ã…œ', 'ã„±', ' ', 'ã…Š', 'ã…š', 'ã„±', 'ã…—', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRMiilvz68AL",
        "outputId": "61fb631b-135a-4d8f-f721-85f721b619a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) JDK ì„¤ì¹˜\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-17-jdk\n",
        "\n",
        "# 2) JPype/KoNLPy ì„¤ì¹˜ (Py3.12 í˜¸í™˜ JPype1 ë²„ì „ ì§€ì •)\n",
        "!pip install -U JPype1==1.5.0 konlpy\n",
        "\n",
        "# 3) í™˜ê²½ë³€ìˆ˜ ì„¤ì • (í˜„ì¬ ì„¸ì…˜ì—ì„œë§Œ ìœ íš¨)\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "# (ì˜µì…˜) í˜¹ì‹œë¥¼ ìœ„í•´ LD_LIBRARY_PATH ì„¤ì •\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = os.environ[\"JAVA_HOME\"] + \"/lib/server:\" + os.environ.get(\"LD_LIBRARY_PATH\",\"\")\n",
        "\n",
        "# 4) JVM ê²½ë¡œ í™•ì¸ (ì •ìƒì´ë¼ë©´ libjvm.so ê²½ë¡œê°€ ì¶œë ¥ë¨)\n",
        "import jpype\n",
        "print(jpype.getDefaultJVMPath())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1T_-DAM7liM",
        "outputId": "71f85a1e-d8a0-406a-8ee0-60904442af22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-17-jdk is already the newest version (17.0.16+8~us1-0ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "Requirement already satisfied: JPype1==1.5.0 in /usr/local/lib/python3.12/dist-packages (1.5.0)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1==1.5.0) (25.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "/usr/lib/jvm/java-17-openjdk-amd64/lib/server/libjvm.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.04 Okt í† í°í™”"
      ],
      "metadata": {
        "id": "_xwwiHKUFTZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "sentence = \"ë¬´ì—‡ì´ë“  ìƒìƒí•  ìˆ˜ ìˆëŠ” ì‚¬ëŒì€ ë¬´ì—‡ì´ë“  ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤.\"\n",
        "\n",
        "nouns = okt.nouns(sentence)\n",
        "phrases = okt.phrases(sentence)\n",
        "morphs = okt.morphs(sentence)\n",
        "pos = okt.pos(sentence)\n",
        "\n",
        "print(\"ëª…ì‚¬ ì¶”ì¶œ :\", nouns)\n",
        "print(\"êµ¬ ì¶”ì¶œ :\", phrases)\n",
        "print(\"í˜•íƒœì†Œ ì¶”ì¶œ :\", morphs)\n",
        "print(\"í’ˆì‚¬ íƒœê¹… :\", pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwjSPA4RNxnA",
        "outputId": "9f60bae9-fdd3-4073-de18-538ae87734cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ëª…ì‚¬ ì¶”ì¶œ : ['ë¬´ì—‡', 'ìƒìƒ', 'ìˆ˜', 'ì‚¬ëŒ', 'ë¬´ì—‡', 'ë‚¼', 'ìˆ˜']\n",
            "êµ¬ ì¶”ì¶œ : ['ë¬´ì—‡', 'ìƒìƒ', 'ìƒìƒí•  ìˆ˜', 'ìƒìƒí•  ìˆ˜ ìˆëŠ” ì‚¬ëŒ', 'ì‚¬ëŒ']\n",
            "í˜•íƒœì†Œ ì¶”ì¶œ : ['ë¬´ì—‡', 'ì´ë“ ', 'ìƒìƒ', 'í• ', 'ìˆ˜', 'ìˆëŠ”', 'ì‚¬ëŒ', 'ì€', 'ë¬´ì—‡', 'ì´ë“ ', 'ë§Œë“¤ì–´', 'ë‚¼', 'ìˆ˜', 'ìˆë‹¤', '.']\n",
            "í’ˆì‚¬ íƒœê¹… : [('ë¬´ì—‡', 'Noun'), ('ì´ë“ ', 'Josa'), ('ìƒìƒ', 'Noun'), ('í• ', 'Verb'), ('ìˆ˜', 'Noun'), ('ìˆëŠ”', 'Adjective'), ('ì‚¬ëŒ', 'Noun'), ('ì€', 'Josa'), ('ë¬´ì—‡', 'Noun'), ('ì´ë“ ', 'Josa'), ('ë§Œë“¤ì–´', 'Verb'), ('ë‚¼', 'Noun'), ('ìˆ˜', 'Noun'), ('ìˆë‹¤', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.05 ê¼¬ê¼¬ë§ˆ í† í°í™”"
      ],
      "metadata": {
        "id": "etAAht2VFTbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "\n",
        "\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = \"ë¬´ì—‡ì´ë“  ìƒìƒí•  ìˆ˜ ìˆëŠ” ì‚¬ëŒì€ ë¬´ì—‡ì´ë“  ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤.\"\n",
        "\n",
        "nouns = kkma.nouns(sentence)\n",
        "sentences = kkma.sentences(sentence)\n",
        "morphs = kkma.morphs(sentence)\n",
        "pos = kkma.pos(sentence)\n",
        "\n",
        "print(\"ëª…ì‚¬ ì¶”ì¶œ :\", nouns)\n",
        "print(\"ë¬¸ì¥ ì¶”ì¶œ :\", sentences)\n",
        "print(\"í˜•íƒœì†Œ ì¶”ì¶œ :\", morphs)\n",
        "print(\"í’ˆì‚¬ íƒœê¹… :\", pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDQf2g_ENyYj",
        "outputId": "573f67fb-9f4e-4dac-99cb-e3e1b5cfd438"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ëª…ì‚¬ ì¶”ì¶œ : ['ë¬´ì—‡', 'ìƒìƒ', 'ìˆ˜', 'ì‚¬ëŒ', 'ë¬´ì—‡']\n",
            "ë¬¸ì¥ ì¶”ì¶œ : ['ë¬´ì—‡ì´ë“  ìƒìƒí•  ìˆ˜ ìˆëŠ” ì‚¬ëŒì€ ë¬´ì—‡ì´ë“  ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤.']\n",
            "í˜•íƒœì†Œ ì¶”ì¶œ : ['ë¬´ì—‡', 'ì´', 'ë“ ', 'ìƒìƒ', 'í•˜', 'ã„¹', 'ìˆ˜', 'ìˆ', 'ëŠ”', 'ì‚¬ëŒ', 'ì€', 'ë¬´ì—‡', 'ì´', 'ë“ ', 'ë§Œë“¤', 'ì–´', 'ë‚´', 'ã„¹', 'ìˆ˜', 'ìˆ', 'ë‹¤', '.']\n",
            "í’ˆì‚¬ íƒœê¹… : [('ë¬´ì—‡', 'NNG'), ('ì´', 'VCP'), ('ë“ ', 'ECE'), ('ìƒìƒ', 'NNG'), ('í•˜', 'XSV'), ('ã„¹', 'ETD'), ('ìˆ˜', 'NNB'), ('ìˆ', 'VV'), ('ëŠ”', 'ETD'), ('ì‚¬ëŒ', 'NNG'), ('ì€', 'JX'), ('ë¬´ì—‡', 'NP'), ('ì´', 'VCP'), ('ë“ ', 'ECE'), ('ë§Œë“¤', 'VV'), ('ì–´', 'ECD'), ('ë‚´', 'VXV'), ('ã„¹', 'ETD'), ('ìˆ˜', 'NNB'), ('ìˆ', 'VV'), ('ë‹¤', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.06 íŒ¨í‚¤ì§€ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"
      ],
      "metadata": {
        "id": "xvseMokGFTd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v26kLuSj8CLc",
        "outputId": "ff2163e1-0f80-45f3-a1b3-2079ef5fedbd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"all\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")"
      ],
      "metadata": {
        "id": "Gmum10nCOEZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.07 ì˜ë¬¸ í† í°í™”"
      ],
      "metadata": {
        "id": "QwNru4gKFThM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "\n",
        "\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "\n",
        "word_tokens = tokenize.word_tokenize(sentence)\n",
        "sent_tokens = tokenize.sent_tokenize(sentence)\n",
        "\n",
        "print(word_tokens)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_6xgZKdOFWS",
        "outputId": "27afb661-dd7d-434d-bbb5-10c6f9dca4b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Those', 'who', 'can', 'imagine', 'anything', ',', 'can', 'create', 'the', 'impossible', '.']\n",
            "['Those who can imagine anything, can create the impossible.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.08 ì˜ë¬¸ í’ˆì‚¬ íƒœê¹…"
      ],
      "metadata": {
        "id": "vsk6aRRKFTj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tag\n",
        "from nltk import tokenize\n",
        "\n",
        "\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "\n",
        "word_tokens = tokenize.word_tokenize(sentence)\n",
        "pos = tag.pos_tag(word_tokens)\n",
        "\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDlpwyueOH99",
        "outputId": "75b81ae6-c3ee-4d94-af60-c4b08fd25b27"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('imagine', 'VB'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.09 spaCy í’ˆì‚¬íƒœê¹…"
      ],
      "metadata": {
        "id": "AYFieruAFTmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8juj631ZHnlD",
        "outputId": "a99356f8-0939-4e4a-a8ab-05e67f859c58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"[{token.pos_:5} - {token.tag_:3}] : {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Skh6UyOKux",
        "outputId": "196ee176-214a-4b8a-c65e-7191c54ec20f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PRON  - DT ] : Those\n",
            "[PRON  - WP ] : who\n",
            "[AUX   - MD ] : can\n",
            "[VERB  - VB ] : imagine\n",
            "[PRON  - NN ] : anything\n",
            "[PUNCT - ,  ] : ,\n",
            "[AUX   - MD ] : can\n",
            "[VERB  - VB ] : create\n",
            "[DET   - DT ] : the\n",
            "[ADJ   - JJ ] : impossible\n",
            "[PUNCT - .  ] : .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.10 ì²­ì™€ëŒ€ ì²­ì› ë°ì´í„° ë‹¤ìš´ë¡œë“œ"
      ],
      "metadata": {
        "id": "aIQBbg6YFTpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5GoGz6n8uhv",
        "outputId": "a85243de-5cea-4a70-e018-f00f808a422c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Korpora in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.12/dist-packages (from Korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.12/dist-packages (from Korpora) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from Korpora) (2.32.4)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->Korpora) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->Korpora) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->Korpora) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->Korpora) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"korean_petitions\")\n",
        "dataset = corpus.train\n",
        "petition = dataset[0]\n",
        "\n",
        "print(\"ì²­ì› ì‹œì‘ì¼ :\", petition.begin)\n",
        "print(\"ì²­ì› ì¢…ë£Œì¼ :\", petition.end)\n",
        "print(\"ì²­ì› ë™ì˜ ìˆ˜ :\", petition.num_agree)\n",
        "print(\"ì²­ì› ë²”ì£¼ :\", petition.category)\n",
        "print(\"ì²­ì› ì œëª© :\", petition.title)\n",
        "print(\"ì²­ì› ë³¸ë¬¸ :\", petition.text[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T1OxLDeOMn3",
        "outputId": "656048c2-058b-4cc2-b503-41e43e169ade"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì´ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì‹  ë§ë­‰ì¹˜ë“¤ì„\n",
            "    ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•©ë‹ˆë‹¤.\n",
            "\n",
            "    ë§ë­‰ì¹˜ë“¤ì„ ê³µìœ í•´ ì£¼ì‹  ë¶„ë“¤ì—ê²Œ ê°ì‚¬ë“œë¦¬ë©°, ê° ë§ë­‰ì¹˜ ë³„ ì„¤ëª…ê³¼ ë¼ì´ì„¼ìŠ¤ë¥¼ ê³µìœ  ë“œë¦½ë‹ˆë‹¤.\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì•„ë˜ì˜ description ì„ ì°¸ê³ ,\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ë¥¼ ì—°êµ¬/ìƒìš©ì˜ ëª©ì ìœ¼ë¡œ ì´ìš©í•˜ì‹¤ ë•Œì—ëŠ” ì•„ë˜ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ê²Œì‹œíŒì˜ ë°ì´í„°ë¥¼ ì›”ë³„ë¡œ ìˆ˜ì§‘í•œ ê²ƒì…ë‹ˆë‹¤.\n",
            "    ì²­ì›ì€ ê²Œì‹œíŒì— ê¸€ì„ ì˜¬ë¦° ë’¤, í•œë‹¬ ê°„ ì²­ì›ì´ ì§„í–‰ë©ë‹ˆë‹¤.\n",
            "    ìˆ˜ì§‘ë˜ëŠ” ë°ì´í„°ëŠ” ì²­ì›ì¢…ë£Œê°€ ëœ ì´í›„ì˜ ë°ì´í„°ì´ë©°, ì²­ì› ë‚´ ëŒ“ê¸€ì€ ìˆ˜ì§‘ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
            "    ë‹¨ ì²­ì›ì˜ ë™ì˜ ê°œìˆ˜ëŠ” ìˆ˜ì§‘ë©ë‹ˆë‹¤.\n",
            "    ìì„¸í•œ ë‚´ìš©ì€ ìœ„ì˜ repositoryë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 23.0MB/s]\n",
            "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 46.9MB/s]                            \n",
            "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 45.8MB/s]                            \n",
            "[korean_petitions] download petitions_2017-11: 28.4MB [00:00, 110MB/s]                             \n",
            "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 185MB/s]                            \n",
            "[korean_petitions] download petitions_2018-01: 43.9MB [00:00, 127MB/s]                            \n",
            "[korean_petitions] download petitions_2018-02: 33.8MB [00:00, 120MB/s]                            \n",
            "[korean_petitions] download petitions_2018-03: 34.3MB [00:00, 198MB/s]                            \n",
            "[korean_petitions] download petitions_2018-04: 35.5MB [00:00, 177MB/s]                            \n",
            "[korean_petitions] download petitions_2018-05: 37.5MB [00:00, 175MB/s]                            \n",
            "[korean_petitions] download petitions_2018-06: 37.8MB [00:00, 236MB/s]                            \n",
            "[korean_petitions] download petitions_2018-07: 40.5MB [00:00, 212MB/s]                            \n",
            "[korean_petitions] download petitions_2018-08: 39.8MB [00:00, 273MB/s]                            \n",
            "[korean_petitions] download petitions_2018-09: 36.1MB [00:00, 262MB/s]                            \n",
            "[korean_petitions] download petitions_2018-10: 38.1MB [00:00, 248MB/s]                            \n",
            "[korean_petitions] download petitions_2018-11: 37.7MB [00:00, 260MB/s]                            \n",
            "[korean_petitions] download petitions_2018-12: 33.0MB [00:00, 246MB/s]                            \n",
            "[korean_petitions] download petitions_2019-01: 34.8MB [00:00, 246MB/s]                            \n",
            "[korean_petitions] download petitions_2019-02: 30.8MB [00:00, 193MB/s]                            \n",
            "[korean_petitions] download petitions_2019-03: 34.9MB [00:00, 219MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì²­ì› ì‹œì‘ì¼ : 2017-08-25\n",
            "ì²­ì› ì¢…ë£Œì¼ : 2017-09-24\n",
            "ì²­ì› ë™ì˜ ìˆ˜ : 88\n",
            "ì²­ì› ë²”ì£¼ : ìœ¡ì•„/êµìœ¡\n",
            "ì²­ì› ì œëª© : í•™êµëŠ” ì¸ë ¥ì„¼í„°, ì·¨ì—…ì„¼í„°ê°€ ì•„ë‹™ë‹ˆë‹¤. ì •ë§ ê°„ê³¡íˆ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\n",
            "ì²­ì› ë³¸ë¬¸ : ì•ˆë…•í•˜ì„¸ìš”. í˜„ì¬ ì‚¬ëŒ€, êµëŒ€ ë“± êµì›ì–‘ì„±í•™êµë“¤ì˜ ì˜ˆë¹„\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.11 í•™ìŠµ ë°ì´í„°ì„¸íŠ¸ ìƒì„±"
      ],
      "metadata": {
        "id": "0Wxm4TTbFTsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"korean_petitions\")\n",
        "petitions = corpus.get_all_texts()\n",
        "with open(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/datasets/corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for petition in petitions:\n",
        "        f.write(petition + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucKhtnfIOPaF",
        "outputId": "b02a7130-0590-4beb-a089-9a93e8ccce5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì´ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì‹  ë§ë­‰ì¹˜ë“¤ì„\n",
            "    ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•©ë‹ˆë‹¤.\n",
            "\n",
            "    ë§ë­‰ì¹˜ë“¤ì„ ê³µìœ í•´ ì£¼ì‹  ë¶„ë“¤ì—ê²Œ ê°ì‚¬ë“œë¦¬ë©°, ê° ë§ë­‰ì¹˜ ë³„ ì„¤ëª…ê³¼ ë¼ì´ì„¼ìŠ¤ë¥¼ ê³µìœ  ë“œë¦½ë‹ˆë‹¤.\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì•„ë˜ì˜ description ì„ ì°¸ê³ ,\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ë¥¼ ì—°êµ¬/ìƒìš©ì˜ ëª©ì ìœ¼ë¡œ ì´ìš©í•˜ì‹¤ ë•Œì—ëŠ” ì•„ë˜ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ê²Œì‹œíŒì˜ ë°ì´í„°ë¥¼ ì›”ë³„ë¡œ ìˆ˜ì§‘í•œ ê²ƒì…ë‹ˆë‹¤.\n",
            "    ì²­ì›ì€ ê²Œì‹œíŒì— ê¸€ì„ ì˜¬ë¦° ë’¤, í•œë‹¬ ê°„ ì²­ì›ì´ ì§„í–‰ë©ë‹ˆë‹¤.\n",
            "    ìˆ˜ì§‘ë˜ëŠ” ë°ì´í„°ëŠ” ì²­ì›ì¢…ë£Œê°€ ëœ ì´í›„ì˜ ë°ì´í„°ì´ë©°, ì²­ì› ë‚´ ëŒ“ê¸€ì€ ìˆ˜ì§‘ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
            "    ë‹¨ ì²­ì›ì˜ ë™ì˜ ê°œìˆ˜ëŠ” ìˆ˜ì§‘ë©ë‹ˆë‹¤.\n",
            "    ìì„¸í•œ ë‚´ìš©ì€ ìœ„ì˜ repositoryë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-08\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-09\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-10\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-11\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-12\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-01\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-02\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-03\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-04\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-05\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-06\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-07\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-08\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-09\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-10\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-11\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-12\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-01\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-02\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.12 í† í¬ë‚˜ì´ì € ëª¨ë¸ í•™ìŠµ"
      ],
      "metadata": {
        "id": "NTdNZ92OFTvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceTrainer\n",
        "\n",
        "\n",
        "SentencePieceTrainer.Train(\n",
        "    \"--input=/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/datasets/corpus.txt\\\n",
        "    --model_prefix=/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/petition_bpe\\\n",
        "    --vocab_size=8000 model_type=bpe\"\n",
        ")"
      ],
      "metadata": {
        "id": "BBauTUl6OU5y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.13 ë°”ì´íŠ¸í˜ì–´ ì¸ì½”ë”© í† í°í™”"
      ],
      "metadata": {
        "id": "vRm_vLHyFTyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "\n",
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/petition_bpe.model\")\n",
        "\n",
        "sentence = \"ì•ˆë…•í•˜ì„¸ìš”, í† í¬ë‚˜ì´ì €ê°€ ì˜ í•™ìŠµë˜ì—ˆêµ°ìš”!\"\n",
        "sentences = [\"ì´ë ‡ê²Œ ì…ë ¥ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì„œ\", \"ì‰½ê²Œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤\"]\n",
        "\n",
        "tokenized_sentence = tokenizer.encode_as_pieces(sentence)\n",
        "tokenized_sentences = tokenizer.encode_as_pieces(sentences)\n",
        "print(\"ë‹¨ì¼ ë¬¸ì¥ í† í°í™” :\", tokenized_sentence)\n",
        "print(\"ì—¬ëŸ¬ ë¬¸ì¥ í† í°í™” :\", tokenized_sentences)\n",
        "\n",
        "encoded_sentence = tokenizer.encode_as_ids(sentence)\n",
        "encoded_sentences = tokenizer.encode_as_ids(sentences)\n",
        "print(\"ë‹¨ì¼ ë¬¸ì¥ ì •ìˆ˜ ì¸ì½”ë”© :\", encoded_sentence)\n",
        "print(\"ì—¬ëŸ¬ ë¬¸ì¥ ì •ìˆ˜ ì¸ì½”ë”© :\", encoded_sentences)\n",
        "\n",
        "decode_ids = tokenizer.decode_ids(encoded_sentences)\n",
        "decode_pieces = tokenizer.decode_pieces(encoded_sentences)\n",
        "print(\"ì •ìˆ˜ ì¸ì½”ë”©ì—ì„œ ë¬¸ì¥ ë³€í™˜ :\", decode_ids)\n",
        "print(\"í•˜ìœ„ ë‹¨ì–´ í† í°ì—ì„œ ë¬¸ì¥ ë³€í™˜ :\", decode_pieces)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH7jdUl1UnTC",
        "outputId": "470ae78c-1da3-4032-ad2e-4fedeb75f0f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì¼ ë¬¸ì¥ í† í°í™” : ['â–ì•ˆë…•í•˜ì„¸ìš”', ',', 'â–í† ', 'í¬', 'ë‚˜', 'ì´', 'ì €', 'ê°€', 'â–ì˜', 'â–í•™', 'ìŠµ', 'ë˜ì—ˆ', 'êµ°ìš”', '!']\n",
            "ì—¬ëŸ¬ ë¬¸ì¥ í† í°í™” : [['â–ì´ë ‡ê²Œ', 'â–ì…', 'ë ¥', 'ê°’ì„', 'â–ë¦¬', 'ìŠ¤íŠ¸', 'ë¡œ', 'â–ë°›ì•„ì„œ'], ['â–ì‰½ê²Œ', 'â–í† ', 'í¬', 'ë‚˜', 'ì´', 'ì €', 'ë¥¼', 'â–ì‚¬ìš©í• ', 'â–ìˆ˜', 'â–ìˆ', 'ë‹µë‹ˆë‹¤']]\n",
            "ë‹¨ì¼ ë¬¸ì¥ ì •ìˆ˜ ì¸ì½”ë”© : [667, 6553, 994, 6880, 6544, 6513, 6590, 6523, 161, 110, 6554, 872, 787, 6648]\n",
            "ì—¬ëŸ¬ ë¬¸ì¥ ì •ìˆ˜ ì¸ì½”ë”© : [[372, 182, 6677, 4433, 1772, 1613, 6527, 4162], [1681, 994, 6880, 6544, 6513, 6590, 6536, 5852, 19, 5, 2639]]\n",
            "ì •ìˆ˜ ì¸ì½”ë”©ì—ì„œ ë¬¸ì¥ ë³€í™˜ : ['ì´ë ‡ê²Œ ì…ë ¥ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì„œ', 'ì‰½ê²Œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤']\n",
            "í•˜ìœ„ ë‹¨ì–´ í† í°ì—ì„œ ë¬¸ì¥ ë³€í™˜ : ['ì´ë ‡ê²Œ ì…ë ¥ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì„œ', 'ì‰½ê²Œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.14 ì–´íœ˜ ì‚¬ì „ ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "WM5WGstjFT1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "\n",
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/petition_bpe.model\")\n",
        "\n",
        "vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}\n",
        "print(list(vocab.items())[:5])\n",
        "print(\"vocab size :\", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62OM-X1WUtD0",
        "outputId": "6af232d4-907b-4e40-deb4-8578a10095d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, 'ë‹ˆë‹¤'), (4, 'â–ì´')]\n",
            "vocab size : 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.15 ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì € í•™ìŠµ"
      ],
      "metadata": {
        "id": "Vk9OZ2asFT4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.normalizers import Sequence, NFD, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(WordPiece())\n",
        "tokenizer.normalizer = Sequence([NFD(), Lowercase()])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "tokenizer.train([\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/datasets/corpus.txt\"])\n",
        "tokenizer.save(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/petition_wordpiece.json\")"
      ],
      "metadata": {
        "id": "nNuQDRp9UwHG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 5.16 ì›Œë“œí”¼ìŠ¤ í† í°í™”"
      ],
      "metadata": {
        "id": "H23jyl2AFT8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer.from_file(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/petition_wordpiece.json\")\n",
        "tokenizer.decoder = WordPieceDecoder()\n",
        "\n",
        "sentence = \"ì•ˆë…•í•˜ì„¸ìš”, í† í¬ë‚˜ì´ì €ê°€ ì˜ í•™ìŠµë˜ì—ˆêµ°ìš”!\"\n",
        "sentences = [\"ì´ë ‡ê²Œ ì…ë ¥ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì„œ\", \"ì‰½ê²Œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤\"]\n",
        "\n",
        "encoded_sentence = tokenizer.encode(sentence)\n",
        "encoded_sentences = tokenizer.encode_batch(sentences)\n",
        "\n",
        "print(\"ì¸ì½”ë” í˜•ì‹ :\", type(encoded_sentence))\n",
        "\n",
        "print(\"ë‹¨ì¼ ë¬¸ì¥ í† í°í™” :\", encoded_sentence.tokens)\n",
        "print(\"ì—¬ëŸ¬ ë¬¸ì¥ í† í°í™” :\", [enc.tokens for enc in encoded_sentences])\n",
        "\n",
        "print(\"ë‹¨ì¼ ë¬¸ì¥ ì •ìˆ˜ ì¸ì½”ë”© :\", encoded_sentence.ids)\n",
        "print(\"ì—¬ëŸ¬ ë¬¸ì¥ ì •ìˆ˜ ì¸ì½”ë”© :\", [enc.ids for enc in encoded_sentences])\n",
        "\n",
        "print(\"ì •ìˆ˜ ì¸ì½”ë”©ì—ì„œ ë¬¸ì¥ ë³€í™˜ :\", tokenizer.decode(encoded_sentence.ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfQj8nsZEyx3",
        "outputId": "ff52bbf3-0a41-42d1-dd84-17c16f8c79da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì¸ì½”ë” í˜•ì‹ : <class 'tokenizers.Encoding'>\n",
            "ë‹¨ì¼ ë¬¸ì¥ í† í°í™” : ['á„‹á…¡á†«á„‚á…§á†¼á„’á…¡á„‰á…¦á„‹á…­', ',', 'á„á…©', '##á„á…³', '##á„‚á…¡á„‹á…µ', '##á„Œá…¥', '##á„€á…¡', 'á„Œá…¡á†¯', 'á„’á…¡á†¨á„‰á…³á†¸', '##á„ƒá…¬á„‹á…¥á†»', '##á„€á…®á†«á„‹á…­', '!']\n",
            "ì—¬ëŸ¬ ë¬¸ì¥ í† í°í™” : [['á„‹á…µá„…á…¥á‡‚á„€á…¦', 'á„‹á…µá†¸á„…á…§á†¨', '##á„€á…¡á†¹á„‹á…³á†¯', 'á„…á…µá„‰á…³á„á…³', '##á„…á…©', 'á„‡á…¡á†®á„‹á…¡á„‰á…¥'], ['á„‰á…±á†¸á„€á…¦', 'á„á…©', '##á„á…³', '##á„‚á…¡á„‹á…µ', '##á„Œá…¥', '##á„…á…³á†¯', 'á„‰á…¡á„‹á…­á†¼á„’á…¡á†¯', 'á„‰á…®', 'á„‹á…µá†»á„ƒá…¡', '##á†¸á„‚á…µá„ƒá…¡']]\n",
            "ë‹¨ì¼ ë¬¸ì¥ ì •ìˆ˜ ì¸ì½”ë”© : [8760, 11, 8693, 8415, 16269, 7536, 7488, 7842, 15016, 8670, 8734, 0]\n",
            "ì—¬ëŸ¬ ë¬¸ì¥ ì •ìˆ˜ ì¸ì½”ë”© : [[8187, 19643, 13834, 28119, 7495, 12607], [9739, 8693, 8415, 16269, 7536, 7510, 14129, 7562, 8157, 7489]]\n",
            "ì •ìˆ˜ ì¸ì½”ë”©ì—ì„œ ë¬¸ì¥ ë³€í™˜ : á„‹á…¡á†«á„‚á…§á†¼á„’á…¡á„‰á…¦á„‹á…­, á„á…©á„á…³á„‚á…¡á„‹á…µá„Œá…¥á„€á…¡ á„Œá…¡á†¯ á„’á…¡á†¨á„‰á…³á†¸á„ƒá…¬á„‹á…¥á†»á„€á…®á†«á„‹á…­!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6ì¥ ì„ë² ë”©"
      ],
      "metadata": {
        "id": "Rh14pQlyEyUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.01 N-gram"
      ],
      "metadata": {
        "id": "v2AQkmWlEx9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "def ngrams(sentence, n):\n",
        "    words = sentence.split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return list(ngrams)\n",
        "\n",
        "sentence = \"ì•ˆë…•í•˜ì„¸ìš” ë§Œë‚˜ì„œ ì§„ì‹¬ìœ¼ë¡œ ë°˜ê°€ì›Œìš”\"\n",
        "\n",
        "unigram = ngrams(sentence, 1)\n",
        "bigram = ngrams(sentence, 2)\n",
        "trigram = ngrams(sentence, 3)\n",
        "\n",
        "print(unigram)\n",
        "print(bigram)\n",
        "print(trigram)\n",
        "\n",
        "unigram = nltk.ngrams(sentence.split(), 1)\n",
        "bigram = nltk.ngrams(sentence.split(), 2)\n",
        "trigram = nltk.ngrams(sentence.split(), 3)\n",
        "\n",
        "print(list(unigram))\n",
        "print(list(bigram))\n",
        "print(list(trigram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiR39SxNU8nu",
        "outputId": "9f51bfa2-10f9-479f-9b86-09182eafa86f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ì•ˆë…•í•˜ì„¸ìš”',), ('ë§Œë‚˜ì„œ',), ('ì§„ì‹¬ìœ¼ë¡œ',), ('ë°˜ê°€ì›Œìš”',)]\n",
            "[('ì•ˆë…•í•˜ì„¸ìš”', 'ë§Œë‚˜ì„œ'), ('ë§Œë‚˜ì„œ', 'ì§„ì‹¬ìœ¼ë¡œ'), ('ì§„ì‹¬ìœ¼ë¡œ', 'ë°˜ê°€ì›Œìš”')]\n",
            "[('ì•ˆë…•í•˜ì„¸ìš”', 'ë§Œë‚˜ì„œ', 'ì§„ì‹¬ìœ¼ë¡œ'), ('ë§Œë‚˜ì„œ', 'ì§„ì‹¬ìœ¼ë¡œ', 'ë°˜ê°€ì›Œìš”')]\n",
            "[('ì•ˆë…•í•˜ì„¸ìš”',), ('ë§Œë‚˜ì„œ',), ('ì§„ì‹¬ìœ¼ë¡œ',), ('ë°˜ê°€ì›Œìš”',)]\n",
            "[('ì•ˆë…•í•˜ì„¸ìš”', 'ë§Œë‚˜ì„œ'), ('ë§Œë‚˜ì„œ', 'ì§„ì‹¬ìœ¼ë¡œ'), ('ì§„ì‹¬ìœ¼ë¡œ', 'ë°˜ê°€ì›Œìš”')]\n",
            "[('ì•ˆë…•í•˜ì„¸ìš”', 'ë§Œë‚˜ì„œ', 'ì§„ì‹¬ìœ¼ë¡œ'), ('ë§Œë‚˜ì„œ', 'ì§„ì‹¬ìœ¼ë¡œ', 'ë°˜ê°€ì›Œìš”')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.02 TF-IDF ê³„ì‚°"
      ],
      "metadata": {
        "id": "nTEBtDnaJvWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"That movie is famous movie\",\n",
        "    \"I like that actor\",\n",
        "    \"I donâ€™t like that actor\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(corpus)\n",
        "tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
        "# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(tfidf_matrix.toarray())\n",
        "print(tfidf_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfaqvsOIVAOU",
        "outputId": "930207a2-57f1-4a9d-e43e-e603e8e6c329"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n",
            "  0.2344005 ]\n",
            " [0.61980538 0.         0.         0.         0.61980538 0.\n",
            "  0.48133417]\n",
            " [0.4804584  0.63174505 0.         0.         0.4804584  0.\n",
            "  0.37311881]]\n",
            "{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.03 ê¸°ë³¸ Skip-gram í´ë˜ìŠ¤"
      ],
      "metadata": {
        "id": "4b__AaWaJvec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class VanillaSkipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=embedding_dim,\n",
        "            out_features=vocab_size\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        output = self.linear(embeddings)\n",
        "        return output"
      ],
      "metadata": {
        "id": "x8jWMHzGVD2B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.04 ì˜í™” ë¦¬ë·° ë°ì´í„°ì„¸íŠ¸ ì „ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "gH1ywB41Jvfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp0f1qGleOPs",
        "outputId": "73593b8d-206e-484a-fd9b-28b15ce88f58"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì´ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì‹  ë§ë­‰ì¹˜ë“¤ì„\n",
            "    ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•©ë‹ˆë‹¤.\n",
            "\n",
            "    ë§ë­‰ì¹˜ë“¤ì„ ê³µìœ í•´ ì£¼ì‹  ë¶„ë“¤ì—ê²Œ ê°ì‚¬ë“œë¦¬ë©°, ê° ë§ë­‰ì¹˜ ë³„ ì„¤ëª…ê³¼ ë¼ì´ì„¼ìŠ¤ë¥¼ ê³µìœ  ë“œë¦½ë‹ˆë‹¤.\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì•„ë˜ì˜ description ì„ ì°¸ê³ ,\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ë¥¼ ì—°êµ¬/ìƒìš©ì˜ ëª©ì ìœ¼ë¡œ ì´ìš©í•˜ì‹¤ ë•Œì—ëŠ” ì•„ë˜ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 165MB/s]\n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 80.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.05 ë‹¨ì–´ì‚¬ì „ êµ¬ì¶•"
      ],
      "metadata": {
        "id": "k7OomE-1JvgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPzlDniAeOm3",
        "outputId": "9e2f6493-8cf0-4b92-842e-eea90b22617c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['êµ³', 'ã…‹'], ['GDNTOPCLASSINTHECLUB'], ['ë­', 'ì•¼', 'ì´', 'í‰ì ', 'ë“¤', 'ì€', '....', 'ë‚˜ì˜ì§„', 'ì•Šì§€ë§Œ', '10', 'ì ', 'ì§œ', 'ë¦¬', 'ëŠ”', 'ë”', 'ë”ìš±', 'ì•„ë‹ˆì–ì•„']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.06 Skip-gramì˜ ë‹¨ì–´ ìŒ ì¶”ì¶œ"
      ],
      "metadata": {
        "id": "0bPlpUmhJvhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6TNHA04ePAC",
        "outputId": "6d2ce572-ce5f-4a62-e5e3-5573f3bcf5dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '.', 'ì´', 'ì˜í™”', 'ì˜', '..', 'ê°€', 'ì—', '...', 'ì„']\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_pairs(tokens, window_size):\n",
        "    pairs = []\n",
        "    for sentence in tokens:\n",
        "        sentence_length = len(sentence)\n",
        "        for idx, center_word in enumerate(sentence):\n",
        "            window_start = max(0, idx - window_size)\n",
        "            window_end = min(sentence_length, idx + window_size + 1)\n",
        "            center_word = sentence[idx]\n",
        "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
        "            for context_word in context_words:\n",
        "                pairs.append([center_word, context_word])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "word_pairs = get_word_pairs(tokens, window_size=2)\n",
        "print(word_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAMI38MjeVjY",
        "outputId": "075831e6-6fdd-4791-f00f-242a3ca16fbe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['êµ³', 'ã…‹'], ['ã…‹', 'êµ³'], ['ë­', 'ì•¼'], ['ë­', 'ì´'], ['ì•¼', 'ë­']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.07 ì¸ë±ìŠ¤ ìŒ ë³€í™˜"
      ],
      "metadata": {
        "id": "2_ExyVYDJviK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index_pairs(word_pairs, token_to_id):\n",
        "    pairs = []\n",
        "    unk_index = token_to_id[\"<unk>\"]\n",
        "    for word_pair in word_pairs:\n",
        "        center_word, context_word = word_pair\n",
        "        center_index = token_to_id.get(center_word, unk_index)\n",
        "        context_index = token_to_id.get(context_word, unk_index)\n",
        "        pairs.append([center_index, context_index])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
        "print(index_pairs[:5])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOG__1UYeXUt",
        "outputId": "fab47e72-1fbf-4179-88fb-5f7314a1c28e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.08 ë°ì´í„°ë¡œë” ì ìš©"
      ],
      "metadata": {
        "id": "1GaV2SqmJvjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "index_pairs = torch.tensor(index_pairs)\n",
        "center_indexes = index_pairs[:, 0]\n",
        "context_indexes = index_pairs[:, 1]\n",
        "\n",
        "dataset = TensorDataset(center_indexes, context_indexes)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "F3Ihmh21eY36"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.09 Skip-gram ëª¨ë¸ ì¤€ë¹„ ì‘ì—…"
      ],
      "metadata": {
        "id": "PJH3YNabJvj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "Rdwg3gmrMVAM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.10 ëª¨ë¸ í•™ìŠµ\n",
        "\n"
      ],
      "metadata": {
        "id": "dlOOtYQbJvk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    cost = 0.0\n",
        "    for input_ids, target_ids in dataloader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        logits = word2vec(input_ids)\n",
        "        loss = criterion(logits, target_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cost += loss\n",
        "\n",
        "    cost = cost / len(dataloader)\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrPSPfU8ecXc",
        "outputId": "3bc02533-bc22-4845-a928-03e6ee06b918"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :    1, Cost : 6.197\n",
            "Epoch :    2, Cost : 5.981\n",
            "Epoch :    3, Cost : 5.932\n",
            "Epoch :    4, Cost : 5.902\n",
            "Epoch :    5, Cost : 5.880\n",
            "Epoch :    6, Cost : 5.862\n",
            "Epoch :    7, Cost : 5.847\n",
            "Epoch :    8, Cost : 5.834\n",
            "Epoch :    9, Cost : 5.823\n",
            "Epoch :   10, Cost : 5.812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.11 ì„ë² ë”© ê°’ ì¶”ì¶œ"
      ],
      "metadata": {
        "id": "gP5OMt9BJvlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_embedding = dict()\n",
        "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, embedding in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = embedding\n",
        "\n",
        "index = 30\n",
        "token = vocab[index]\n",
        "token_embedding = token_to_embedding[token]\n",
        "print(token)\n",
        "print(token_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-L5pEneeeXO",
        "outputId": "009fcea9-1d94-44b7-b83f-529b4087ebfd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì—°ê¸°\n",
            "[-0.33981228 -1.0527135  -0.4362162  -0.7486608   0.54111713  0.06911109\n",
            "  1.0284543  -0.25667953 -2.034866    0.72982955  0.5620684  -0.6126388\n",
            "  0.82970685 -3.422404   -0.7440557   0.15919691 -0.2813956   0.26970768\n",
            " -0.5235333  -0.8207271   0.15284528  0.89040875 -0.63450396 -1.2289039\n",
            "  0.05851755  1.519074    0.53663653 -1.8141434   0.6933153   1.4336888\n",
            "  1.071853    1.236509    0.415109    1.120217    0.79168004 -0.94725364\n",
            " -0.7465214   0.2620171  -0.55633235 -0.6240903  -0.09001063  0.82584834\n",
            " -0.52528286 -1.1229595   1.6634943  -0.8049566  -0.7738365   0.7988896\n",
            " -0.9329564   1.1756132  -0.17071427 -1.1013689  -0.24914904 -0.76050687\n",
            "  1.8833514  -0.43316486  0.172486    0.21582922 -0.95688826 -0.39478722\n",
            " -1.4003313   0.851508   -0.7759602   0.6434033   0.12873323  0.59406126\n",
            " -1.5460391   0.5113602  -0.32450932 -1.1695704  -1.3411943  -0.11437356\n",
            " -0.12054121  0.1701726  -0.35119528 -0.06835467  0.26910487 -0.77061564\n",
            " -0.05542282  1.8031293   0.44745886 -1.3237208   0.6551376  -0.01652435\n",
            " -0.2079932   1.5163163   0.17195974 -0.03193222  0.23465969 -1.1232307\n",
            "  0.18749098  0.91975987 -1.2312958  -0.06171717  2.2540133  -1.1600709\n",
            "  0.4094319   0.7559785   1.1247243  -0.46543854  0.89799696  0.20976597\n",
            " -0.18498947 -0.6358651  -0.9870104   0.14613785 -0.1519232   0.24088801\n",
            " -0.7935895  -0.7522631   0.01273232 -0.27735135 -0.21282755 -0.98904914\n",
            " -1.8430358   0.02547671 -0.05921721  0.25718668 -0.41103473  0.05356555\n",
            "  0.25125006 -0.12396321  0.736257    0.69556737  1.1420529  -0.31537694\n",
            "  1.882899    0.7019035 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.12 ë‹¨ì–´ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°"
      ],
      "metadata": {
        "id": "jI1uKjVTJvmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
        "    return cosine\n",
        "\n",
        "def top_n_index(cosine_matrix, n):\n",
        "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
        "    top_n = closest_indexes[1 : n + 1]\n",
        "    return top_n\n",
        "\n",
        "\n",
        "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
        "top_n = top_n_index(cosine_matrix, n=5)\n",
        "\n",
        "print(f\"{token}ì™€ ê°€ì¥ ìœ ì‚¬í•œ 5 ê°œ ë‹¨ì–´\")\n",
        "for index in top_n:\n",
        "    print(f\"{id_to_token[index]} - ìœ ì‚¬ë„ : {cosine_matrix[index]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxF0-kHjeifD",
        "outputId": "a0b3d7cd-7d25-4e14-a8aa-eba9c75fe138"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì—°ê¸°ì™€ ê°€ì¥ ìœ ì‚¬í•œ 5 ê°œ ë‹¨ì–´\n",
            "íƒœêµ­ - ìœ ì‚¬ë„ : 0.2863\n",
            "í—ˆì¤€ - ìœ ì‚¬ë„ : 0.2687\n",
            "ë“±ì¥ì¸ë¬¼ - ìœ ì‚¬ë„ : 0.2677\n",
            "ë° - ìœ ì‚¬ë„ : 0.2622\n",
            "ì¿¡ - ìœ ì‚¬ë„ : 0.2619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.13 Word2Vec ëª¨ë¸ í•™ìŠµ"
      ],
      "metadata": {
        "id": "jxLRdRJtJvnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSazRa3Eel_n",
        "outputId": "8c205072-57f2-4874-9652-16bd2cb20d0f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì´ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì‹  ë§ë­‰ì¹˜ë“¤ì„\n",
            "    ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•©ë‹ˆë‹¤.\n",
            "\n",
            "    ë§ë­‰ì¹˜ë“¤ì„ ê³µìœ í•´ ì£¼ì‹  ë¶„ë“¤ì—ê²Œ ê°ì‚¬ë“œë¦¬ë©°, ê° ë§ë­‰ì¹˜ ë³„ ì„¤ëª…ê³¼ ë¼ì´ì„¼ìŠ¤ë¥¼ ê³µìœ  ë“œë¦½ë‹ˆë‹¤.\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì•„ë˜ì˜ description ì„ ì°¸ê³ ,\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ë¥¼ ì—°êµ¬/ìƒìš©ì˜ ëª©ì ìœ¼ë¡œ ì´ìš©í•˜ì‹¤ ë•Œì—ëŠ” ì•„ë˜ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]"
      ],
      "metadata": {
        "id": "SHDjZOh4enWa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouNP609kFT8Q",
        "outputId": "ee6fd0d0-48f9-4e1c-e7c3-7c2dbf89323d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "word2vec = Word2Vec(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    max_final_vocab=10000\n",
        ")"
      ],
      "metadata": {
        "id": "Q3TXsQl_epsS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "word2vec.save(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/word2vec.model\")\n",
        "word2vec = Word2Vec.load(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/word2vec.model\")"
      ],
      "metadata": {
        "id": "SQo3agAGerY0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.14 ì„ë² ë”© ì¶”ì¶œ ë° ìœ ì‚¬ë„ ê³„ì‚°"
      ],
      "metadata": {
        "id": "Zh_2jt0YJvoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"ì—°ê¸°\"\n",
        "print(word2vec.wv[word])\n",
        "print(word2vec.wv.most_similar(word, topn=5))\n",
        "print(word2vec.wv.similarity(w1=word, w2=\"ì—°ê¸°ë ¥\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo7qe_3ves1q",
        "outputId": "ae10270e-cddb-481c-f4b5-bbfd4de3914c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.08937192 -0.21536446  0.4713405   0.48026463  0.05583267 -0.09758369\n",
            "  0.17404887 -0.14441341 -0.5242054   0.19269422 -0.17730004 -0.27681923\n",
            " -0.16770504 -0.03193996 -0.35377932 -0.01700783 -0.3752685   0.3684339\n",
            " -0.19465853  0.49840173  0.78031635  0.6262023  -0.21024619 -0.20324416\n",
            " -0.15485243  0.05155503 -0.36064473  0.12933661  0.12836206 -0.23449576\n",
            " -0.36395332  0.22667651  0.16381389 -0.15535754 -0.15624525 -0.4889851\n",
            "  0.23177597 -0.04568569 -0.02460453 -0.43961748 -0.19575703  0.3678515\n",
            " -0.21795548 -0.40654826 -0.0761662   0.42301044 -0.2335839  -0.34971386\n",
            "  0.08472556  0.05725687  0.76314026  0.25161457  0.00454403  0.34753218\n",
            " -0.4903567  -0.09974898  0.09590919  0.2221917  -0.2576437   0.06020138\n",
            "  0.00438794 -0.17555058  0.10912545  0.09009639 -0.2120871  -0.04049155\n",
            "  0.04388328  0.24582668  0.42013803 -0.28850117 -0.31307352 -0.28495833\n",
            " -0.42545465  0.04121992 -0.20644069  0.02354049 -0.21432038 -0.42594576\n",
            " -0.23607469  0.49133754 -0.11293259 -0.08315013  0.21149978  0.7684034\n",
            "  0.14325841  0.37392026  0.10172487 -0.20890327  0.03795305  0.06633201\n",
            " -0.09367175  0.04898977  0.04431826  0.07192627  0.29248202  0.02813354\n",
            "  0.04523049  0.03589857 -0.23387066 -0.3612738  -0.21850975 -0.38236466\n",
            "  0.33679348 -0.42700276 -0.00824314 -0.09531332  0.51504546  0.1789005\n",
            " -0.05505378 -0.31915265  0.28866333 -0.07069977 -0.03975021  0.18465228\n",
            "  0.22027893 -0.1934877   0.40767807  0.31874925  0.02392494  0.08570486\n",
            " -0.0965003  -0.11308871  0.1440646  -0.00104957 -0.1322829  -0.0247116\n",
            " -0.43281034 -0.34226698]\n",
            "[('ì—°ê¸°ë ¥', 0.7989179491996765), ('ìºìŠ¤íŒ…', 0.7473562359809875), ('ëª¸ë§¤', 0.7091493010520935), ('ì¡°ì—°', 0.7030120491981506), ('ì—°ê¸°ì', 0.6979253888130188)]\n",
            "0.7989179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.15 KorNLI ë°ì´í„°ì„¸íŠ¸ ì „ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "dgTbOAKvJvoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"kornli\")\n",
        "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n",
        "tokens = [sentence.split() for sentence in corpus_texts]\n",
        "\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyxjzqade1WA",
        "outputId": "0113b309-108e-4a1b-ff07-068544a1a254"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì´ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì‹  ë§ë­‰ì¹˜ë“¤ì„\n",
            "    ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•©ë‹ˆë‹¤.\n",
            "\n",
            "    ë§ë­‰ì¹˜ë“¤ì„ ê³µìœ í•´ ì£¼ì‹  ë¶„ë“¤ì—ê²Œ ê°ì‚¬ë“œë¦¬ë©°, ê° ë§ë­‰ì¹˜ ë³„ ì„¤ëª…ê³¼ ë¼ì´ì„¼ìŠ¤ë¥¼ ê³µìœ  ë“œë¦½ë‹ˆë‹¤.\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì•„ë˜ì˜ description ì„ ì°¸ê³ ,\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ë¥¼ ì—°êµ¬/ìƒìš©ì˜ ëª©ì ìœ¼ë¡œ ì´ìš©í•˜ì‹¤ ë•Œì—ëŠ” ì•„ë˜ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
            "\n",
            "    # Description\n",
            "    Author : KakaoBrain\n",
            "    Repository : https://github.com/kakaobrain/KorNLUDatasets\n",
            "    References :\n",
            "        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n",
            "           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n",
            "           (https://arxiv.org/abs/2004.03289)\n",
            "\n",
            "    This is the dataset repository for our paper\n",
            "    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n",
            "    (https://arxiv.org/abs/2004.03289)\n",
            "    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n",
            "\n",
            "    # License\n",
            "    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n",
            "    Details in https://creativecommons.org/licenses/by-sa/4.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[kornli] download multinli.train.ko.tsv: 83.6MB [00:00, 295MB/s]                            \n",
            "[kornli] download snli_1.0_train.ko.tsv: 78.5MB [00:00, 231MB/s]                            \n",
            "[kornli] download xnli.dev.ko.tsv: 516kB [00:00, 12.5MB/s]\n",
            "[kornli] download xnli.test.ko.tsv: 1.04MB [00:00, 23.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['ê°œë…ì ìœ¼ë¡œ', 'í¬ë¦¼', 'ìŠ¤í‚¤ë°ì€', 'ì œí’ˆê³¼', 'ì§€ë¦¬ë¼ëŠ”', 'ë‘', 'ê°€ì§€', 'ê¸°ë³¸', 'ì°¨ì›ì„', 'ê°€ì§€ê³ ', 'ìˆë‹¤.'], ['ì‹œì¦Œ', 'ì¤‘ì—', 'ì•Œê³ ', 'ìˆëŠ”', 'ê±°', 'ì•Œì•„?', 'ë„¤', 'ë ˆë²¨ì—ì„œ', 'ë‹¤ìŒ', 'ë ˆë²¨ë¡œ', 'ìƒì–´ë²„ë¦¬ëŠ”', 'ê±°ì•¼', 'ë¸Œë ˆì´ë¸ŒìŠ¤ê°€', 'ëª¨íŒ€ì„', 'ë– ì˜¬ë¦¬ê¸°ë¡œ', 'ê²°ì •í•˜ë©´', 'ë¸Œë ˆì´ë¸ŒìŠ¤ê°€', 'íŠ¸ë¦¬í”Œ', 'Aì—ì„œ', 'í•œ', 'ë‚¨ìë¥¼', 'ë– ì˜¬ë¦¬ê¸°ë¡œ', 'ê²°ì •í•˜ë©´', 'ë”ë¸”', 'Aê°€', 'ê·¸ë¥¼', 'ëŒ€ì‹ í•˜ëŸ¬', 'ì˜¬ë¼ê°€ê³ ', 'A', 'í•œ', 'ëª…ì´', 'ê·¸ë¥¼', 'ëŒ€ì‹ í•˜ëŸ¬', 'ì˜¬ë¼ê°„ë‹¤.'], ['ìš°ë¦¬', 'ë²ˆí˜¸', 'ì¤‘', 'í•˜ë‚˜ê°€', 'ë‹¹ì‹ ì˜', 'ì§€ì‹œë¥¼', 'ì„¸ë°€í•˜ê²Œ', 'ìˆ˜í–‰í• ', 'ê²ƒì´ë‹¤.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.16 fastText ëª¨ë¸ ì‹¤ìŠµ"
      ],
      "metadata": {
        "id": "QMF30X2yJ__-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "\n",
        "fastText = FastText(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    max_final_vocab=20000,\n",
        "    epochs=3,\n",
        "    min_n=2,\n",
        "    max_n=6\n",
        ")\n",
        "fastText.save(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/fastText.model\")\n",
        "fastText = FastText.load(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/fastText.model\")"
      ],
      "metadata": {
        "id": "n1x5uEjie5ES"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.17 fastText OOV ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "-KKRoG90KAH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oov_token = \"ì‚¬ë‘í•´ìš”\"\n",
        "oov_vector = fastText.wv[oov_token]\n",
        "\n",
        "print(oov_token in fastText.wv.index_to_key)\n",
        "print(fastText.wv.most_similar(oov_vector, topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D_K8Jdle6wF",
        "outputId": "bb170b38-7ce2-4078-80af-4c1572b33be0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "[('ì‚¬ë‘', 0.8698300719261169), ('ì‚¬ë‘ì—', 0.837310254573822), ('ì‚¬ë‘ì˜', 0.7879202365875244), ('ì‚¬ë‘ì„', 0.761152982711792), ('ì‚¬ë‘í•˜ëŠ”', 0.7559911608695984)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.18 ì–‘ë°©í–¥ ë‹¤ì¸µ ì‹ ê²½ë§"
      ],
      "metadata": {
        "id": "iPPkTmzhKAI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "ouput_size = 256\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "\n",
        "model = nn.RNN(\n",
        "    input_size=input_size,\n",
        "    hidden_size=ouput_size,\n",
        "    num_layers=num_layers,\n",
        "    nonlinearity=\"tanh\",\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "sequence_len = 6\n",
        "\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
        "h_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, ouput_size)\n",
        "\n",
        "outputs, hidden = model(inputs, h_0)\n",
        "print(outputs.shape)\n",
        "print(hidden.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utdWBcB3fBsN",
        "outputId": "6fbc543b-a26b-4e64-839f-a361e831d981"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 6, 512])\n",
            "torch.Size([6, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.19 ì–‘ë°©í–¥ ë‹¤ì¸µ ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬"
      ],
      "metadata": {
        "id": "hH8VfGxIKAJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "input_size = 128\n",
        "output_size = 256\n",
        "num_layers = 3\n",
        "bidirectional = True\n",
        "proj_size = 64\n",
        "\n",
        "model = nn.LSTM(\n",
        "    input_size=input_size,\n",
        "    hidden_size=output_size,\n",
        "    num_layers=num_layers,\n",
        "    batch_first=True,\n",
        "    bidirectional=bidirectional,\n",
        "    proj_size=proj_size,\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "sequence_len = 6\n",
        "\n",
        "inputs = torch.randn(batch_size, sequence_len, input_size)\n",
        "h_0 = torch.rand(\n",
        "    num_layers * (int(bidirectional) + 1),\n",
        "    batch_size,\n",
        "    proj_size if proj_size > 0 else output_size,\n",
        ")\n",
        "c_0 = torch.rand(num_layers * (int(bidirectional) + 1), batch_size, output_size)\n",
        "\n",
        "outputs, (h_n, c_n) = model(inputs, (h_0, c_0))\n",
        "\n",
        "print(outputs.shape)\n",
        "print(h_n.shape)\n",
        "print(c_n.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoo_RYMOfE41",
        "outputId": "53cf9e4b-64fa-4828-e76a-cac9073b2352"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 6, 128])\n",
            "torch.Size([6, 4, 64])\n",
            "torch.Size([6, 4, 256])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: LSTM with projections is not supported with oneDNN. Using default implementation. (Triggered internally at /pytorch/aten/src/ATen/native/RNN.cpp:1473.)\n",
            "  result = _VF.lstm(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.20 ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸"
      ],
      "metadata": {
        "id": "G-C9P9yIKAK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab,\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        if model_type == \"rnn\":\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.classifier(last_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "3i5mb6WYfMWp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.21 ë°ì´í„°ì„¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "eIjOsR5KKMFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RfHmZPTfVOB",
        "outputId": "4f73c30a-6586-4b7c-8a3f-1d2b57335eb4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì´ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì‹  ë§ë­‰ì¹˜ë“¤ì„\n",
            "    ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•©ë‹ˆë‹¤.\n",
            "\n",
            "    ë§ë­‰ì¹˜ë“¤ì„ ê³µìœ í•´ ì£¼ì‹  ë¶„ë“¤ì—ê²Œ ê°ì‚¬ë“œë¦¬ë©°, ê° ë§ë­‰ì¹˜ ë³„ ì„¤ëª…ê³¼ ë¼ì´ì„¼ìŠ¤ë¥¼ ê³µìœ  ë“œë¦½ë‹ˆë‹¤.\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì•„ë˜ì˜ description ì„ ì°¸ê³ ,\n",
            "    í•´ë‹¹ ë§ë­‰ì¹˜ë¥¼ ì—°êµ¬/ìƒìš©ì˜ ëª©ì ìœ¼ë¡œ ì´ìš©í•˜ì‹¤ ë•Œì—ëŠ” ì•„ë˜ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = corpus_df.sample(frac=0.9, random_state=42)\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data Size :\", len(train))\n",
        "print(\"Testing Data Size :\", len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tacxEHqfYDk",
        "outputId": "c7736c3b-4f9a-4b61-c8b2-357cbb51fd54"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|       | text                                                                                     |   label |\n",
            "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
            "| 33553 | ëª¨ë“  í¸ê²¬ì„ ë‚ ë ¤ ë²„ë¦¬ëŠ” ê°€ìŠ´ ë”°ëœ»í•œ ì˜í™”. ë¡œë²„íŠ¸ ë“œ ë‹ˆë¡œ, í•„ë¦½ ì„¸ì´ëª¨ì–´ í˜¸í”„ë§Œ ì˜ì›í•˜ë¼. |       1 |\n",
            "|  9427 | ë¬´í•œ ë¦¬ë©”ì´í¬ì˜ ì†Œì¬. ê°ë…ì˜ ì—­ëŸ‰ì€ í•­ìƒ ê·¸ ìë¦¬ì—...                                    |       0 |\n",
            "|   199 | ì‹ ë‚  ê²ƒ ì—†ëŠ” ì• ë‹ˆ.                                                                       |       0 |\n",
            "| 12447 | ì”ì” ê²©ë™                                                                                |       1 |\n",
            "| 39489 | ì˜¤ëœë§Œì— ì°¾ì€ ì£¼ë§ì˜ ëª…í™”ì˜ ë³´ì„                                                         |       1 |\n",
            "Training Data Size : 45000\n",
            "Testing Data Size : 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.22 ë°ì´í„° í† í°í™” ë° ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•"
      ],
      "metadata": {
        "id": "Jj95uQscKMIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab = build_vocab(corpus=train_tokens, n_vocab=5000, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVOzewyifciy",
        "outputId": "4ef5e04d-807d-42b8-9b32-280715df7cf1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<unk>', '.', 'ì´', 'ì˜í™”', 'ì˜', '..', 'ê°€', 'ì—', '...']\n",
            "5002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.23 ì ìˆ˜ ì¸ì½”ë”© ë° íŒ¨ë”©"
      ],
      "metadata": {
        "id": "B2XfY5uUKMLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:\n",
        "        sequence = sequence[:max_length]\n",
        "        pad_length = max_length - len(sequence)\n",
        "        padded_sequence = sequence + [pad_value] * pad_length\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in train_tokens\n",
        "]\n",
        "test_ids = [\n",
        "    [token_to_id.get(token, unk_id) for token in review] for review in test_tokens\n",
        "]\n",
        "\n",
        "max_length = 32\n",
        "pad_id = token_to_id[\"<pad>\"]\n",
        "train_ids = pad_sequences(train_ids, max_length, pad_id)\n",
        "test_ids = pad_sequences(test_ids, max_length, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsxC3H7GffuO",
        "outputId": "1618fd49-75a7-4d04-ec8c-5c0612aac470"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
            " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
            "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.24 ë°ì´í„°ë¡œë” ì ìš©"
      ],
      "metadata": {
        "id": "2ib9GqlAKMN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "oCkrpiEqfjRZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.25 ì†ì‹¤í•¨ìˆ˜ì™€ ìµœì í™” í•¨ìˆ˜ ì •ì˜"
      ],
      "metadata": {
        "id": "U6JqpBdCKMQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZABKdafjzf",
        "outputId": "8a5dd597-ffc6-4e38-d2e6-d16c827b9d50"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3652411983.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_ids = torch.tensor(train_ids)\n",
            "/tmp/ipython-input-3652411983.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_ids = torch.tensor(test_ids)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.26 ëª¨ë¸ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸"
      ],
      "metadata": {
        "id": "H58i1ka4KMWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocab = len(token_to_id)\n",
        "hidden_dim = 64\n",
        "embedding_dim = 128\n",
        "n_layers = 2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "QLmcgDRofn6s"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f\"Train Loss {step} : {np.mean(losses)}\")"
      ],
      "metadata": {
        "id": "tu0bpeZXfqjV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, datasets, criterion, device):\n",
        "    model.eval()\n",
        "    losses = list()\n",
        "    corrects = list()\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        yhat = torch.sigmoid(logits)>.5\n",
        "        corrects.extend(\n",
        "            torch.eq(yhat, labels).cpu().tolist()\n",
        "        )\n",
        "\n",
        "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")"
      ],
      "metadata": {
        "id": "ZcdXTXUEfr3a"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtcrcfJJfuCP",
        "outputId": "086dd720-127c-4460-a6d9-f1ae549e1df9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss 0 : 0.6901999115943909\n",
            "Train Loss 500 : 0.6941046597001082\n",
            "Train Loss 1000 : 0.692830226995371\n",
            "Train Loss 1500 : 0.6811580586679612\n",
            "Train Loss 2000 : 0.6696147570188257\n",
            "Train Loss 2500 : 0.6596562811395065\n",
            "Val Loss : 0.607028952897928, Val Accuracy : 0.6934\n",
            "Train Loss 0 : 0.60292649269104\n",
            "Train Loss 500 : 0.566380674194195\n",
            "Train Loss 1000 : 0.5556957794652952\n",
            "Train Loss 1500 : 0.5462306047542186\n",
            "Train Loss 2000 : 0.5329119414910265\n",
            "Train Loss 2500 : 0.5195462569886329\n",
            "Val Loss : 0.4427871796460197, Val Accuracy : 0.7866\n",
            "Train Loss 0 : 0.3239092528820038\n",
            "Train Loss 500 : 0.4075262514714471\n",
            "Train Loss 1000 : 0.4062475398346618\n",
            "Train Loss 1500 : 0.4037667020539853\n",
            "Train Loss 2000 : 0.4037108086366048\n",
            "Train Loss 2500 : 0.4022967993307476\n",
            "Val Loss : 0.40053051881515944, Val Accuracy : 0.8144\n",
            "Train Loss 0 : 0.2510043978691101\n",
            "Train Loss 500 : 0.3523946141084273\n",
            "Train Loss 1000 : 0.34974155434361703\n",
            "Train Loss 1500 : 0.34978940160690664\n",
            "Train Loss 2000 : 0.34828210132627535\n",
            "Train Loss 2500 : 0.3488465870340697\n",
            "Val Loss : 0.3895904176627485, Val Accuracy : 0.8164\n",
            "Train Loss 0 : 0.17636790871620178\n",
            "Train Loss 500 : 0.30622670025436466\n",
            "Train Loss 1000 : 0.30343492439651226\n",
            "Train Loss 1500 : 0.3064515230994793\n",
            "Train Loss 2000 : 0.31002943063790595\n",
            "Train Loss 2500 : 0.313556782715097\n",
            "Val Loss : 0.40476853509966176, Val Accuracy : 0.8226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.27 í•™ìŠµëœ ëª¨ë¸ë¡œë¶€í„° ì„ë² ë”© ì¶”ì¶œ"
      ],
      "metadata": {
        "id": "59Vz6SKEKMZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_embedding = dict()\n",
        "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, emb in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = emb\n",
        "\n",
        "token = vocab[1000]\n",
        "print(token, token_to_embedding[token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpZyQT8Zfv19",
        "outputId": "167f3f79-fa9e-48a3-a918-645e34f245dd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë³´ê³ ì‹¶ë‹¤ [ 0.0626849   0.40081125  0.8560834  -0.42082977  0.24151747  1.0149747\n",
            " -0.9385682  -0.71223676  0.27913782 -0.24204856 -0.5927462   1.3532274\n",
            " -0.36877456  0.65721947 -1.0235441   1.0758637  -0.48012972 -0.77523154\n",
            " -0.78512746  0.13437995  0.9120595   0.7665408  -0.79701734 -0.16305831\n",
            " -0.6820582  -1.0536164   0.49272025 -0.65174246 -1.7046881   0.27139345\n",
            "  0.6273749  -0.17295022  0.3706571  -1.6759387  -0.5108245  -1.2963909\n",
            "  0.54608786  0.57686716 -1.0231125  -0.22181924  0.10609841 -1.8949916\n",
            " -0.78318727 -1.806044   -0.89788836 -0.32910442 -0.94411224  1.1952254\n",
            "  0.4637042   1.5668589  -0.76098716 -0.900374   -1.8919077   1.1846088\n",
            " -0.3734085   2.3169186   0.82512563  0.6668274   0.55426794  0.19276614\n",
            "  0.6235291  -1.5167253   0.6951185  -1.4751396   0.661146    0.8667369\n",
            "  0.3341795   2.3234255  -0.48690942 -1.2936386   1.9254688   0.59462655\n",
            " -0.545033   -1.9064764   1.3626871  -0.44004586 -1.8025031  -1.7230515\n",
            " -1.6312605   0.34802628  0.9422363   0.03613015 -1.2154975  -0.6775962\n",
            "  0.1724827   0.62005955  1.7962117  -0.5661123  -0.48548436 -0.05224117\n",
            " -0.34070894  0.08694798 -1.6204487   2.2298288  -1.2363122  -0.22898532\n",
            " -0.12814927  1.2686169  -1.1394076  -1.0882155  -1.3136017  -1.0093905\n",
            "  0.21979067 -1.1000658   0.8115875   0.33005044 -0.40081993 -0.07564081\n",
            "  0.4414624   0.06344973 -0.19168575 -0.44857633  0.32445228  1.0905248\n",
            " -0.5046294   0.66731834 -0.40078634 -0.50230235 -0.369227    1.12685\n",
            " -0.07906384  0.57269704 -1.2673413  -0.2570002   0.07236598 -1.0257759\n",
            "  0.05393302 -0.69053566]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.28 ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë¡œ ì„ë² ë”© ê³„ì¸µ ì´ˆê¸°í™”"
      ],
      "metadata": {
        "id": "X7ZjNpGPKMd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "word2vec = Word2Vec.load(\"/content/drive/MyDrive/Euron_9thDL/pytorch_transformer/models/word2vec.model\")\n",
        "init_embeddings = np.zeros((n_vocab, embedding_dim))\n",
        "\n",
        "for index, token in id_to_token.items():\n",
        "    if token not in [\"<pad>\", \"<unk>\"]:\n",
        "        init_embeddings[index] = word2vec.wv[token]\n",
        "\n",
        "embedding_layer = nn.Embedding.from_pretrained(\n",
        "    torch.tensor(init_embeddings, dtype=torch.float32)\n",
        ")"
      ],
      "metadata": {
        "id": "PpmqORFggLQ0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.29 ì‚¬ì „í•™ìŠµëœ ì„ë² ë”© ê³„ì¸µ ì ìš©"
      ],
      "metadata": {
        "id": "hSYI4MrsKMrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab,\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\",\n",
        "        pretrained_embedding=None ##\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if pretrained_embedding is not None: ##\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "            )\n",
        "        else:\n",
        "            self.embedding = nn.Embedding( ##\n",
        "                num_embeddings=n_vocab,\n",
        "                embedding_dim=embedding_dim,\n",
        "                padding_idx=0\n",
        "            )\n",
        "\n",
        "        if model_type == \"rnn\":\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                bidirectional=bidirectional,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.classifier(last_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "P_UfIeqxgSEo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.30 ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”©ì„ ì‚¬ìš©í•œ ëª¨ë¸ í•™ìŠµ"
      ],
      "metadata": {
        "id": "ypnj7FR6LPpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim,\n",
        "n_layers=n_layers, pretrained_embedding=init_embeddings\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_gNuHIzgrVI",
        "outputId": "8edc0d35-2ba4-4284-9c26-c8241594b0a9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss 0 : 0.6880645751953125\n",
            "Train Loss 500 : 0.6752193027032826\n",
            "Train Loss 1000 : 0.62280240580514\n",
            "Train Loss 1500 : 0.5912846927242545\n",
            "Train Loss 2000 : 0.5692630030851493\n",
            "Train Loss 2500 : 0.5552357761014323\n",
            "Val Loss : 0.46894482268502535, Val Accuracy : 0.7596\n",
            "Train Loss 0 : 0.2682529389858246\n",
            "Train Loss 500 : 0.4811739302323964\n",
            "Train Loss 1000 : 0.4822555576617663\n",
            "Train Loss 1500 : 0.480032395693082\n",
            "Train Loss 2000 : 0.4771013436713319\n",
            "Train Loss 2500 : 0.47742253555352093\n",
            "Val Loss : 0.442840858532217, Val Accuracy : 0.7928\n",
            "Train Loss 0 : 0.4861535131931305\n",
            "Train Loss 500 : 0.4592318024166568\n",
            "Train Loss 1000 : 0.4597809752384266\n",
            "Train Loss 1500 : 0.4605757931246113\n",
            "Train Loss 2000 : 0.45914849911493877\n",
            "Train Loss 2500 : 0.45570273093226815\n",
            "Val Loss : 0.45916342725769016, Val Accuracy : 0.779\n",
            "Train Loss 0 : 0.4576092064380646\n",
            "Train Loss 500 : 0.4382452356541704\n",
            "Train Loss 1000 : 0.44243919757100847\n",
            "Train Loss 1500 : 0.43975130039243043\n",
            "Train Loss 2000 : 0.44123931556776247\n",
            "Train Loss 2500 : 0.43999778721533694\n",
            "Val Loss : 0.4513416955360589, Val Accuracy : 0.7908\n",
            "Train Loss 0 : 0.648817777633667\n",
            "Train Loss 500 : 0.4362150103210689\n",
            "Train Loss 1000 : 0.43416077714342693\n",
            "Train Loss 1500 : 0.43130228345807914\n",
            "Train Loss 2000 : 0.4305783617293936\n",
            "Train Loss 2500 : 0.4292234282155649\n",
            "Val Loss : 0.42396633588848787, Val Accuracy : 0.8016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.31 í•©ì„±ê³± ëª¨ë¸"
      ],
      "metadata": {
        "id": "_l97QQ2pLTjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(32 * 32 * 32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "q7JJ963NNpsP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.32 í•©ì„±ê³± ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸ ì •ì˜"
      ],
      "metadata": {
        "id": "Saz1gB_3LTmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_embedding, filter_sizes, max_length, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(pretrained_embedding, dtype=torch.float32)\n",
        "        )\n",
        "        embedding_dim = self.embedding.weight.shape[1]\n",
        "\n",
        "        conv = []\n",
        "        for size in filter_sizes:\n",
        "            conv.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(\n",
        "                        in_channels=embedding_dim,\n",
        "                        out_channels=1,\n",
        "                        kernel_size=size\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool1d(kernel_size=max_length-size-1),\n",
        "                )\n",
        "            )\n",
        "        self.conv_filters = nn.ModuleList(conv)\n",
        "\n",
        "        output_size = len(filter_sizes)\n",
        "        self.pre_classifier = nn.Linear(output_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(output_size, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        embeddings = embeddings.permute(0, 2, 1)\n",
        "        conv_outputs = [conv(embeddings) for conv in self.conv_filters]\n",
        "        concat_outputs = torch.cat([conv.squeeze(-1) for conv in conv_outputs], dim=1)\n",
        "\n",
        "        logits = self.pre_classifier(concat_outputs)\n",
        "        logits = self.dropout(logits)\n",
        "        logits = self.classifier(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "dwUDHIojg5sJ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì˜ˆì œ 6.33 í•©ì„±ê³± ì‹ ê²½ë§ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ"
      ],
      "metadata": {
        "id": "MwCLx3VELTpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_xBMGf0BEhRj"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "filter_sizes = [3, 3, 4, 4, 5, 5]\n",
        "classifier = SentenceClassifier(\n",
        "    pretrained_embedding=init_embeddings,\n",
        "    filter_sizes=filter_sizes,\n",
        "    max_length=max_length\n",
        ").to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "    test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm2UXU2zhetW",
        "outputId": "3fef92b2-54f9-40ae-fb43-9fb3644cf42d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss 0 : 0.675045371055603\n",
            "Train Loss 500 : 0.5687502534268145\n",
            "Train Loss 1000 : 0.5393593186116242\n",
            "Train Loss 1500 : 0.5240296673607937\n",
            "Train Loss 2000 : 0.5176196219860346\n",
            "Train Loss 2500 : 0.5124521354278151\n",
            "Val Loss : 0.45929043080669624, Val Accuracy : 0.7794\n",
            "Train Loss 0 : 0.6370831727981567\n",
            "Train Loss 500 : 0.4855229981585653\n",
            "Train Loss 1000 : 0.481467748453448\n",
            "Train Loss 1500 : 0.4780320257127166\n",
            "Train Loss 2000 : 0.4781429448957624\n",
            "Train Loss 2500 : 0.47859071088475924\n",
            "Val Loss : 0.4517983596641035, Val Accuracy : 0.7848\n",
            "Train Loss 0 : 0.6824628114700317\n",
            "Train Loss 500 : 0.4759096468161204\n",
            "Train Loss 1000 : 0.47322067268542595\n",
            "Train Loss 1500 : 0.4718802725272048\n",
            "Train Loss 2000 : 0.47120847764133156\n",
            "Train Loss 2500 : 0.47076772745610806\n",
            "Val Loss : 0.44896112825162116, Val Accuracy : 0.786\n",
            "Train Loss 0 : 0.4715694487094879\n",
            "Train Loss 500 : 0.45599888880809625\n",
            "Train Loss 1000 : 0.4599905832932069\n",
            "Train Loss 1500 : 0.4630000906475856\n",
            "Train Loss 2000 : 0.4651670258293028\n",
            "Train Loss 2500 : 0.4641604649393809\n",
            "Val Loss : 0.44847980222572537, Val Accuracy : 0.786\n",
            "Train Loss 0 : 0.28898459672927856\n",
            "Train Loss 500 : 0.46739176349963496\n",
            "Train Loss 1000 : 0.46514050147452435\n",
            "Train Loss 1500 : 0.46278602372281635\n",
            "Train Loss 2000 : 0.461940999286643\n",
            "Train Loss 2500 : 0.4618698994799263\n",
            "Val Loss : 0.4409423640932138, Val Accuracy : 0.7894\n"
          ]
        }
      ]
    }
  ]
}